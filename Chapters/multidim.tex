\chapter{Multidimensionality analysis} \label{chap:multidim} 

When an area of content-related items is pervaded by more than a single latent dimension, one speaks of \textsc{multidimensionality}. Multidimensionality expands the concept of unidimensionality literally to multiple dimensions. This is done, assuming all (sub-)dimensions act simultaneously to produce a particular structure \parencite[pp. 36]{Jacoby1991}. The so-called \textsc{assumption of equal contributions} will be of much greater interest in the next chapter. For now, the question is how many dimensions underlay a particular set of content-related indicators. Unlike the unidimensional case where justifying the absence or presence of a single dimension was prior, now multiple scenarios are plausible (e.\,g., $k = 1, 2, 3, \dots, K$).

To pick up on common ground, let’s view how different multidimensional scenarios translate to the correlation matrix (${R}$). Remember why this is important; a correlation matrix is a place where a latent dimension leaves its mark. Hence, any tool for latent-variable inference extracts empirical arguments from parts of the correlation matrix. Cronbach’s alpha, for example, draws heavily on the average inter-item correlation. In the following, factor analysis will even try to reproduce common parts of it. The correlation matrix is fundamental and understanding its patterns is essential to grasp the flaws and limits of the tools falling back on it.

With this in mind, recall the unidimensional pattern from chapter 3. A single underlying latent dimension ($\zeta$) implies a consistently high correlation among the indicators ($X$). So \textsc{non-dimensionality} is the opposite, consistently low correlations among the indicators (\autoref{fig:weak}). In an extreme case, the correlation matrix equals the identity matrix: ${R} = {I}$. The identity matrix (${I}$) is a matrix with ones in the on-diagonals and zeros in the off-diagonals. This is important to know, because if the correlation matrix equals the identity matrix (${R} = {I}$), items do not correlate with each other ($\rho_{i, j} = 0$).

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/weak-1.png}
\caption[Non-Dimensional Item Response Pattern]{Non-Dimensional Item Response Pattern -- 12 simulated variables sharing few attributes. The item-factor correlation is $\lambda_{i=1,\dots,12} = 0.1$}
\label{fig:weak}
\end{figure}

Despite the unidimensional case, the correlation matrix can hold multiple highly correlating item bundles: $k=1,2,\dots,K$. The pattern is shown in \autoref{fig:mixed} and a mix of the aforementioned. Assembling multiple unidimensional patterns, filling the gaps with the ones previously shown, the correlation matrix is characterized by high within-group correlations and low between-group correlations: 
\begin{equation}
\rho_{x_{i|k}, x_{j|k}} \gg \rho_{x_{i|k},x_{j|l}} 
\end{equation}
The three item response patterns outlined so far cover a wide of circumstances in applied research. But especially the last one ($\rho_{x_{i|k}, x_{j|k}} \gg \rho_{x_{i|k},x_{j|l}}$) will be of great importance in the upcoming chapters. It determines the ease of latent-variable inference in large data sets: \textit{The more a structure looses the in-between-group pattern, the harder to identify the clusters and the harder to infer a latent dimension}. Why? Well, assume the contrary; if the between-group variation decreases, clusters resemble one another and become more equal. This means, their boundaries dissolve until the pattern vanishes:
\begin{equation}
\rho_{x_{i|k},x_{j|k}} \approx \rho_{x_{i|k},x_{j|l}}  
\end{equation}
The problem with such a pattern is that neither the robot nor the researcher can attribute given influences to a particular latent dimension. This hinders inference, because  clusters cannot be meaningfully distinguished.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/mixed-1.png}
\caption[Between-Within-Group Pattern]{Between-Within-Group Pattern -- 12 simulated variables with average loading $\bar\lambda = 0.7$ and average between-factor correlation $\bar\phi=0.45$}
\label{fig:mixed}
\end{figure}

Now recall the OUI’s established characteristics. This so-called \textsc{vanishing effect} is exactly how the blurry boundary feature translates to the correlation matrix. Since latent dimensions often have common attributes, they share information, and the influence on an indicator cannot be uniquely assigned to any of them in particular. The last statement substantiates why, from an empirical point of view, complex social phenomena are hard to separate. Due to their characteristic features, they produce structural patterns which can be hardly distinguished. Those, in turn, hamper the possibility to tell latent dimensions apart. However, a method trying to master both obstacles (i.\,e., vanishing effect and large data set challenge) is factor analysis \endnote{As a little side note; the true score approach of CTT relates closely to exploratory factor analysis. \textcite{Mair2018}, for example, interlinks both approaches in equation 1.1 and 2.2.}.

\section{Factor analysis}

Demarcating factor analysis (FA) mainly forces to separate it from principal component analysis (PCA). PCA is mainly used for descriptive purposes and metric data. It is superior when aiming to minimize the loss of information, compressing a set of variables (${X_{i=1,\dots,m}}$). But in the social sciences, focus of the investigation is merely always on meaningful compression rather than full compression \parencite{Revelle2021}. This is why social scientists are nearly always concerned with \textsc{compression in meaningful ways}; that is to compress and infer latent dimensions. The principle will be expanded in the researcher’s perspective on FA. For now, let it just be said, the common factor model is the formal basis of FA and will be subsequently specified in the fundamental theorem. It is the more suitable option for the undertaking, which is why the focus will be fully on FA\endnote{There are several differences between PCA and FA. For instance, PCA tries to minimize the loss of compression when reproducing information in the correlation matrix ($R$). But the question is if researchers should really expect the variables to capture all the information in the correlation matrix. Or, to put it another way, if the variables can fully explain the variation in the data. In the social sciences, it seems more realistic to model only the *common* features of a sample. That’s where common factor analysis (FA) comes in. FA tries to explain the \textit{common} parts of the correlation matrix; which means (1) covariation (i.\,e., off-diagonals) plus (2) the so-called \textsc{communalities}. The communalities are the common portion of the variance in the on-diagonals. What further distinguished the common factor model (CFM) from PCA is the leftover, called “uniqueness”. The uniqueness holds unexplainable bits, things like measurement error and item-specific variation. These are usually suppressed in standard PCA models. Against the background of the complex measurement (i.e., being doomed to measure with error) CFM’s advantages over PCA seem convincing. One major reason for the confusion between PCA and FA arises from PCM (i.e., principal component method) or PFM (i.\,e., principal factor method). The method will be part of later sections. For now, simply memorize, there is a difference between both approaches (i.\,e., PCA and FA) and a particular extraction method within the approach (i.\,e., PCM and PFA).}.

Factor analysis comes in two major flavors: confirmatory and exploratory. The \textsc{confirmatory} version (CFA) is used to test theory-implied relationships about the common factor model. If, for example, the culture of honor phenomenon is assumable a construct of four sub-dimensions (e.\,g., masculine honor, feminine honor, social honor, and family honor; 
\cite[see][]{Mosquera2008, Souza2017}), researchers can test this proposed structure against data. One can also say, the researcher tries to \textit{confirm} a proposed structure with the data at hand\endnote{\textcite{Brown2015} gives a comprehensive and applied introduction in his book.}. In this thesis, however, the exploratory version takes center stage.

\textsc{Exploratory factor analysis} (EFA) can be intuitively understood as determining the number of factors in an \enquote{exploratory manner} \parencite{Mair2018}. The starting point of any EFA is the correlation matrix (${R}$). Even though improvements in visualization allow to survey the structure of the correlation matrix, most of the time researchers draw on algorithmic procedures to meet the large data set challenge. Nowadays, the most comprehensively implemented algorithmic procedure is factor analysis. Since researchers usually hand work over to an algorithm, the analogy of a robot is used in the following. So think of factor analysis as commissioning robots. In particular, the robot’s task is to help the researcher inspecting the correlation matrix since his or her capabilities of visual inspections are often quickly reached. To tune the search procedure, researchers direct the robot with a particular set of instructions. Some of them will be discussed in the chapter and checked for compliance with the OUI in the social sciences. But first, let’s carve out the researcher’s and the robot’s perspective on factor analysis.

\subsection{Researcher’s perspective}

To get into factor analysis, it is helpful to understand its aim in the social sciences. In applied social research, FA is mainly used to infer latent dimensions (${\zeta}$). This should remind the reader of the latent variable logic from chapter 2. However, now the view will be more specific on factor analysis and how the procedure contributes to this goal.

Which role does the factor-analytic approach play in latent-variable inference? Well, FA tries to provide the researcher with an empirical argument for (at least) one latent dimension. Replicating the common information of the correlation matrix, the key argument is a reduced set of synthetic variables ($\xi$) called \textsc{factors}. In other words, they summarize the monotonic inter-item relationships in a condensed factorial form. The job of the researcher is now to infer a latent dimension from factors. This is done by assumption. Assuming that the latent dimension causes a given pattern in the correlation matrix, most of the inter-item correlations are assumably not random, but results from a common generative process -- the latent dimension. Hence, each factor stores some of the common features in the correlation matrix, which labels the factor a parsimonious meaningful summary of the observed response patterns. Conversely, a specific pattern is often enough for latent variable inference -- assuming the presence of the latent dimension.

Moreover, latent-variable inference using FA allows graduating beyond an indicator (e.\,g., \enquote{You would praise a man that acts aggressively in an insult}) towards an encompassing social construct (e.\,g., honor). Thus, the result of an FA (${\xi}$) will be said to be theoretically more important than the input product (${X}$). In conclusion, \textit{factor analysis compresses the input meaningfully to exceed its meaning in the result}:

\begin{equation}
X_{i=1,\dots,m} \overset{comp.}{\longrightarrow} \xi_{k: k \ll m} \overset{exce.}{\longrightarrow}\zeta
\end{equation}

This is at the heart of \textsc{latent-variable inference}. In the light of the previous finding, factor analysis is indeed a latent variable approach in compliance with the principle of compression in meaningful ways.

\subsection{Robot’s perspective}

The last section outlined the process of utilizing factors for latent-variable inference from a researcher’s perspective. However, there is one crucial point missing -- the robot’s perspective. Recall, researchers usually use algorithmically driven factorial robots to search of highly correlating item groups. Accordingly, researchers have to understand how to commission their robot for a proper search. Being able to supervise a robot and critically evaluate the quality of its results is key in applied research, as \textcite[p. 165]{Gregory2014} emphasizes:

\begin{displayquote}
\enquote{Unfortunately, factor analysis is frequently misunderstood and often misused. Some researchers appear to use factor analysis as a kind of divining rod, hoping to find gold hidden underneath tons of dirt. But there is nothing magical about the technique. No amount of statistical analysis can rescue data based on trivial, irrelevant, or haphazard measures. If there is no gold to be found, then none will be found; factor analysis is not alchemy. Factor analysis will yield meaningful results only when the research was meaningful to begin with.}
\end{displayquote}
The quote prompts towards the GIGO perspective on factor analysis -- garbage in, garbage out. Researchers need to think intensively about the quality of their data to produce feasible inferences. Factor analysis cannot and won’t protect inferences from poor data. Apparently, \textcite{Gregory2014} nudges to understand FA to get the magic out of the procedure. His statement goes hand in hand with the aforementioned perspective on dimensionality analysis. Namely, to grasp the problem and evaluate one’s defaults to alleviate bias. With this in mind, let’s get into the nuts and bolts of factor analysis, understanding the procedure by taking the robot’s view on FA:
\begin{equation}
P \rightarrow R
\end{equation}
The formula is nothing more than a formal instruction to tackle above’s inference problem. Factor analytic robots ask how to smoothly replicate the common information in the correlation matrix ($R$) using a model ($P$) that owns a reduced set of synthetic variables. A suitable example to solve the problem is called the \textsc{common factor model} (CFM). CFM is specified in the so-called \textsc{fundamental equation} of factor analysis -- and merely a reformulation of the input equation $X_i = \Lambda \xi + \epsilon$ \textcite{Mair2018}:
\begin{equation} 
P = \Lambda \Phi \Lambda^{t} + \Psi
\end{equation}

Let’s take this one step at a time, trying to understand both equations by designating the known bits. View both equations simply as (1) a formal outline of components (e.\,g., $P, \Lambda, X_i, \xi$) which are (2) available to the robot to (3) tackle above’s replication issue ($X_{i=1} \rightarrow \xi_{k} \rightarrow\zeta$) from (4) a specific point of view ($P \rightarrow R$). With this in mind, look at both equations:

First, in the input equation ($X_i = \Lambda \xi + \epsilon$) one can surmise the vector of observable indicators ($X_i$), which stores participant’s responses (e.\,g., \enquote{“strongly agree,\dots, strongly disagree}) to questions, like \enquote{You would praise a man that acts aggressively to an insult}. Additionally, \enquote{honor} is part of ${\xi}$ -- the vector of factors. Second, ${\Lambda}$ is of interest. It is the matrix of factor loadings, administering the strength of relationship between factors and items. To demystify this terminology, instead of saying an item \enquote{loads on}, one can say \enquote{correlates with} a given factor \parencite[p. 159]{Gregory2014}. In this sense, a loading ($\lambda_i \in \Lambda$) of let’s say $\lambda_i = 0.7$, implies that the item \enquote{You would praise a man that reacts aggressively to an insult} correlates highly with \enquote{honor}. If the item does not strongly prompt towards any other factor, it can be tagged as a good indicator for the latent dimension. In other words, \enquote{honor} manifests strongly in the item \enquote{You would praise a man that reacts aggressively to an insult}, because both correlate strongly ($\lambda_i = \rho_{X_i, \xi_j}$). Since the matrix of factor loadings furthermore includes all item-factor relations it will prove as a handy tool in later evaluations.

To summarize, factor analysis offers a concrete formal solution for the intricate problem in the social sciences. Indeed, it extends the correlation approach from chapter 3 to meet the large data set challenge. Why? Because the common factor model puts the researcher in the position of using observable indicators to get a vital clue for latent dimensions -- factors. The inference equation can thus be re-specified in light of previous findings:

\begin{equation}
X_{i=1,\dots,m} \overset{P}{\longrightarrow} \xi_{k: k \ll m} \overset{Asm.}{\longrightarrow}\zeta
\end{equation}

Put into words, using the common factor model ($P$) the data ($X_i$) are meaningfully compressed and become factors ($\xi$) which are assumed ($Asm.$) to be empirical indications for the latent dimension ($\zeta$). Now, there is evidence on how to solve the intricate problem within FA. In this light, it is worth shifting attention towards factor analysis. Specifically, the upcoming sections will lie out how the researcher-robot duo tackles the intricate problem in the social sciences. As a result, answer will be given on how to combat the problem of identifying latent dimensions. But before one must face three major problems.

\subsection{Three major problems}

Tackling the intricate problem within FA implies wrangling the three major problems. First, one has to overcome the communality problem and choose an appropriate method to extract factors. Then, there is the rotation problem, which imposes a burden of choosing between dozens of implementations. Another battle in social sciences is the choice between orthogonal rotation and oblique transformations. However, the toughest nut to crack is the number of factor problem, which literally requires answering the tricky question of how many factors to retain. Along the way, many other obstacles, like sample size and interpretation, await the researcher. Especially the latter hurdle applies to theory development because chosen indicators have to be meaningfully upgraded or inferred. All of this will become clear throughout the chapter.

Nonetheless, under the aforementioned outline (exploratory) factor analysis first seems like a hurdle race and the question pops up why FA is the go-to method for exploratory investigations in the social sciences. But undoubtedly it is; and it has proved widely applicable across multiple circumstances in applied research \parencite{Brillinger2004, Dwivedi2006, Majors2001, Tang2000} and disciplines \parencite{Cox2001, Pitombo2011, Williams2010, Yang2003}. Thus, reasons must be found to justify its dominance. 

A trivial one may be the absence of decent alternatives. chapter 5 and 6 deal with this consideration. But if decent alternatives are indeed missing, researchers should clear the hurdles. That is, familiarizing themselves with well-suited solutions for the aforementioned obstacles. Conversely, FA can be an optimal method to learn  from sources of incomplete information -- data. The last-mentioned perspective is taken on now. Furthermore, over four decades of applied research offer a rich pool of critics and alternatives to cope with each aforesaid obstacle. But concerning the OUI, skepticism must be held high along the way, and researchers need to be critical of any proposed solutions. Carefully each procedure must be assessed to prevent method blindness fallacy, and avoid ignorance bias.

\subsubsection{Communality problem}

The {communality problem} results from the common factor model’s indeterminacy. More precisely, the problem arises from the indeterminacy of the fundamental equation, which defines the model. An equation is said to be \textsc{indeterminate} when it holds more unknowns than knowns. To give an example, the equation, $2 + y = x$ is indeterminate. It holds more unknowns (i.\,e., $x$ and $y$) than knowns (i.\,e., $2$). The problem with an indeterminate model is it cannot be solved. One can also say, there is no closed-form solution, but a range of possibilities (e.\,g., $x=2 \vee 3, y=0 \vee 1$).

The common factor model, as specified by the fundamental equation, contains more unknown bits than knowns and is thus indeterminate \parencite{Schonemann1972, SchOnemann1978}. The known bits are the observed item responses on the indicators, which are embedded in the correlation matrix ($R$). The unknown bits are the matrix of factor loadings and the communalities. Both will be defined and elaborated in the following sections. But for now, just note and memorize two things: First, following the stated definition, the common factor model is indeterminate. Second, the listed components (e.\,g., factor loadings and communalities) are the ingredients of the common factor model (${P}$), which is capable to tackle the replication issue ($P \rightarrow R$). But so far, no answer was given on why indeterminacy is problematic. \textcite[p. 24]{Mair2018} puts the communality issue in a nutshell: 
\begin{displayquote}
\enquote{The problem is that in order to estimate the communalities, we need the loadings. Conversely, in order to estimate the loadings, we need the communalities.}
\end{displayquote}

\textcite{Mair2018} proposes to \textit{estimate} the solution; and an estimator will be shown which yields the most plausible values to reproduce the values in the correlation matrix. But an alternative strategy has to be discussed in advance. It is still possible to solve any indeterminate equation -- namely by assumption. If a researcher sets $y=3$, in the indeterminate equation $2 + y = x$  the equation can be solved. Assuming $y=3$, the closed-form solution is $5$. This is a toy example, but it suffices to get the problem and options to solve the upcoming extraction problem. Let’s dive deeper into the topic, starting with the robot’s perspective.

\paragraph{Robot’s perspective}

As \textcite{Mair2018} notes, solving the communality issue means estimating an indeterminate common factor model. As he said, two components are of great importance: the matrix of factor loadings (${\Lambda}$) and, in addition, the communalities. The \textsc{communalities} are the first part at the right-hand side of the fundamental equation ($P = \Lambda \Phi \Lambda^{t} + \Psi$) and define the exploratory power of the common factor model. In addition, there is another component, ${\Psi}$, which is called \textsc{uniqueness}. The uniqueness captures the unexplainable bits -- more precisely, item-specific variation and measurement error. The uniqueness thus represents information leftovers in the replication process.

Let’s resume with the problem; since the common factor model is indeterminate, the robot cannot simply solve the fundamental equation. Focusing on the definition of the communalities while recalling \textcite{Mair2018}'s outline, the problem becomes obvious: The loadings are an inevitable ($\Lambda$) part of the communalities ($\Lambda \Phi \Lambda^{t}$). To provide a result, the robot has two strategies: (1) keep and estimate or (2) simplify and solve. So what to do? In the first case, a factorial robot estimates the most plausible values for the loading matrix ($\hat{{\Lambda}}$) and the communalities ($\hat{{\Lambda}} {\Phi} \hat{{\Lambda}}^t$) given the data. In the second case, it neglects between-factor correlations ($\Phi$) to \enquote{determinate} the equation to return a result. In this case, the communalities simplify to $\Lambda \Lambda^{t}$\endnote{$\Lambda \Phi \Lambda^{t}$ is a common way to define the communalities \parencite{Yong2013}. Literally, the expression reads as \enquote{(sum of the) squared loadings}. The definition follows from CFM under the assumption of independence Assuming independent factor forces their correlation to be zero: $\forall\phi_{i=1,\dots,m} \in \Phi: \phi_{i \neq j} \Rightarrow \phi_{i,j} = 0$ Accordingly, $\Phi$ becomes the identity matrix: $I$. The identity matrix contains $1$’s in the on-diagonals and $0$’s in the off-diagonals. It is the neutral element of matrix multiplication. As a consequence, any matrix $M$ multiplied with $I$ is $M\times I = M$. Thus, if $\Psi = I$, the right-hand side of the fundamental equation $\Lambda \Phi \Lambda^{t}$ renders to $\Lambda \Lambda^{t}$}. The two strategies are what the robot offers -- choosing one of them is a matter for the researcher.

\paragraph{Researcher’s perspective}

Above’s discussion identified the primary concern of the communality issue: the model’s indeterminacy. Recall, indeterminacy is a property of an equation that applies to the fundamental equation and imposes the problem to calculate a model containing more unknowns than knowns. From a researcher’s perspective, tackling the communality problem now requires \enquote{nothing more than} selecting one of the aforementioned strategies under which a solution will be provided. The first approach simplifies the fundamental equation by assumption (i.\,e., neglecting between-factor correlations). In the second approach, the researcher commissions the robot to estimate the most plausible solution given the data.

The problem seems easy to overcome, but over four decades of researchers proved the communality issue to be a complicated problem. Care must be taken when choosing a method to extract factors because the second approach smuggles (factor) independence into the analysis. But assuming independence is problematic since the assumption is incompatible with the OUI in the social sciences. Moreover, assuming independence predetermines the result at an early stage of the investigation. Researchers actually deprive themselves of the possibility to find out about factor independence. In any case, independence is involved. So all proposed solutions must be critically assessed and the reader is advised to watch out for method-blindness fallacy and expect ignorance bias. 

\paragraph{Solutions}

The following two important approaches in applied research are outlined: (1) principal axis factor analysis (PAFA) and (2) maximum likelihood factor analysis (MLFA). PAFA will be a relevant topic of discussion for a single reason. It uses the principal component method (PCM) to provide a closed-form solution for the aforementioned problem under the assumption of factorial independence Literally, that’s what it takes to \textit{solve} the fundamental equation. PAFA is the classic way of extracting factors\endnote{The principal component method (PCM) is the reason for major confusion about FA and PCA. Commercial software like SPSS choose the classic their default extraction method in EFA. As a consequence, singular value or eigenvalue decomposition and factor extraction became intertwined. But they shouldn’t go hand in hand. Stata and R, for instance, take the distinction into account. Stata, separate models within tabs. Likewise, R includes them in different packages. The next sections are devoted to the problem of PCM in social sciences. In recourse, it may become clear why SPSS chose a bad default.}. It dominated the research landscape in the 1980s, before more accurate and resource-hungry methods like MLFA received entry in the social sciences \parencite[see e.\,g.,][]{Ueberla1971}.

\subparagraph{Bad defaults}

The crux of \textsc{principal axis factor analysis} (PAFA) is the idea of maximizing \enquote{information reconnaissance}. Thereby, factors are extracted successively, keeping as much structural information from a reduced correlation matrix (${R}^*$) as possible. For the sake of completeness, the \textsc{reduced matrix} is just the ordinary correlation matrix (${R}$) in which values in the diagonals are replaced with communalities \parencite{Revelle2021}. If one sets the communalities to ${1}$, the reduced matrix equals the original matrix (${R} = {R}^*$)\endnote{Experimenting with the communality values allows understanding the differences and similarities between PAFA and PCA. To get a better intuition of their relation, \textcite{Revelle2021} nudged to play the \textsc{communality game}: In PAFA, the communalities can be determined. If the researcher iterative increases the value to one, PAFA and PCA yield approximately equal results. If the communalities equal ${1}$, PCA can be said to be identical to PAFA. Why? First, on one hand decompose the underlying correlation matrix with an \textsc{eigenvalue decomposition} (or the input matrix with a singular-value decomposition). So they are equal regarding that fact. But on the other hand they use different variants of correlation matrices. Principal component analysis applies the eigenvalue decomposition to the whole correlation matrix ${R}$, whereas PAFA uses the reduced matrix. In simplified terms, they relate to each other as follows: $diag(R) = \mathbf{1}$; $diag(R^*) < \mathbf{1}$. Accordingly, if the reduced matrix iterates its diagonal towards $1$, both resemble one another: $diag(R^*) \rightarrow 1 \Rightarrow R^{*} \approx {R}$. Lastly, as already mentioned, both use an EVD to decompose their matrices. Applying the EVD to an arbitrary correlation matrix ($R$) yields: $R = U \Lambda U^t$ \parencite{Mair2018}. $U$ is a left singular matrix, $U^t$ the transposed right-singular matrix and $\Lambda$ the factor loading matrix. A principal component $C$ can now be defined as product of the singular matrix and the root of the eigenvectors ($\sqrt\lambda$): $C = U\sqrt{\lambda}$ \parencite{Revelle2021}. This simplifies $R$ to the product of two component matrices: $R=CC^t$. Accordingly, for the reduced matrix: $R^*=FF^t$. To summarize, if the correlation matrices resemble one another, so do their results: $diag(R^*) \rightarrow 1 \Rightarrow FF^t \approx CC^t$. This is in line with the findings of \textcite{Harris1964}, for example, who showed that PCA and PAFA provide almost identical results under the principal extraction method. In conclusion, if the correlation matrix resembles one another, the results are equal. If they do not, the results differ \parencite[p. 129]{Gorsuch2015}.}.

However, more important for the following discussion is the extraction procedure: the \textsc{principal component method} (PCM). Think of PCM as drawing out liquid with syringes from a glass of water. In the first step, one absorbs as much of the water (i.\,e., information) from the glass (i.\,e., reduced matrix) as possible. So the first syringe (i.\,e., first factor) comprises as much water from the glass (i.\,e., information from the reduced correlation matrix) as possible. After extracting the first syringe, the second one (i.\,e., factor two) absorbs as much of the remaining water drops from the glass as possible. Thus, the second factor processes the left-out information of factor one. This procedure is repeated until $k$ filled syringes are on the table and no more water (i.e., information) is in the glass (i.\,e., reduced matrix). Now it is obvious why the method can be said to minimize the loss in compression. It maximizes the information density (i.\,e., water) within each factor (i.\,e., syringe)\endnote{Technically speaking, the procedure provides a solution that successively maximizes the (squared) factor-item correlations: $\Lambda \Lambda^{t}$ As a result, the between factor correlations are literally thrown out. Why? Think of the syringes. Maximizing the information density within each factor minimizes the information shared between factors. All soak water from a common source of information. One must decide how to distribute the water across syringes (i.\,e., information across factors). PCM’s choice is, once more, maximizing information reconnaissance. Ultimately, the robots maximize the sum of squares of the factor structure and thus the information kept from the residual matrix after the first factor has been extracted \parencite[p. 101]{Gorsuch2015}}.

However, concerning the OUI, the procedure is problematic. It produces independent factors \parencite{Rummel1967}. Notice that the second syringe does not contain any drop of water which is already present in syringe one – vice versa. Thus, as the absorbed information is independent, so are the factors\endnote{Conceptually, factor independence can be understood following fundamental theorem: $P=\Lambda\Lambda^t + \Psi$. Again, assuming factor independence, the correlation between factors is 0. The matrix of between factor correlations becomes the identity matrix: $\Phi = I$. Under the assumption, the model equation simplifies to $P = \Lambda\Lambda^t + \Psi$. Researchers can easily get rid of the uniqueness, too. Expecting the variables to pick up all the information in the correlation matrix, the uniqueness drops out and $P$ renders to $R$. Why? Because the previous assumption necessitates to get all the information in the correlation matrix -- not just the common ones. Accordingly, $P$ simplifies twice: $R = \Lambda\Lambda^t$. At any case, the explanation suffice understanding the process. However, the description misses a concise argument. Independence actually results from EVD and comes in the form of $U$: $R = U \Lambda U^t$. Remember, $U$ is a singular matrix. The singular matrix comprises orthogonal vectors, which are independent of each other. In sum, independence results from how the information are decomposed. It furthermore expresses in the result, as it was shown with the syringes.}.

But usually, there is no reason for the researcher to assume independence between factors. This holds especially
for the default in an early stage of an exploratory investigation. Remember, the OUI tends towards blurry boundaries. Latent dimensions usually share some bits of information among factors. Referring to examples like \enquote{honor} and \enquote{pride}, or \enquote{nationalism} and \enquote{patriotism}, factorial independence is an exertion rather than a rule. Forcing factors to be independent often disrespects the characteristic features of the OUI in the social sciences. Ruthless application of this procedure is not harmless. It distorts results and can prove as a recipe for doom in applied research \parencite{Hayton2004, Taherdoost2014}. In the end, it is reasonable to label it a bad default and its widespread use reveals a method-blindness fallacy that furthermore imposes ignorance bias in applied social science research \parencite{Heise1973}. This urges switching to decent alternatives. 

\subparagraph{Decent alternatives}

The decent alternative mentioned hereafter is \textsc{Maximum Likelihood Factor Analysis} (MLFA). This is a vastly different and a more complicated approach to solve the communality problem. Here, the aim is to provide a conceptual understanding rather than fleshing out the mathematical nuances. This section is outsourced\endnote{Technically, the ML problem is solved by generalizing the ordinary least squares approach \parencite{Joreskog1978}. In fact, the robot searches for a parameter vector $\theta$ so that $\mathrm{S}(\theta) \rightarrow \min \frac{1}{2} tr(SP^{-1} - I)^2$ ($S$: sample correlation matrix). Minimizing the expression can be shown to maximizing the likelihood of the solution given the data \parencite{Revelle2021}. For this reason, maximum likelihood estimation yields the most plausible values ($\theta$) to reproduce the common parts of the structure in the correlation matrix.}.

Conceptually, MLFA is pretty tangible. With the data at hand, the robot tries to estimate the most plausible (i.\,e., the most likely) values that are capable to optimally reproduce the common information in the (reduced) correlation matrix. In a more approach-based description, one can also say it solves the problem by maximizing the likelihood of the data \parencite{Revelle2021}. The robot is furthermore commissioned under the assumption of multivariate normality. So if the assumption of multivariate normal-distributed residuals is reasonable, maximum likelihood estimates are an attractive solution for the reproduction problem.

\paragraph{Research recommendations}

\textcite{Ueberla1971} already mentioned the principal component method as a go-to method in applied research. He also noted some of its flaws, especially the problem of independence. Almost twenty years later, however, \textcite{Hayton2004} and \textcite{Taherdoost2014} found the problem of independence still a topical subject in applied work. Researchers still resort to methods, inducing independence by default. The widespread use of bad defaults indicate method-blindness fallacy, imposing ignorance bias. Again, in many cases, the OUI is incompatible with the assumption. Similarly, it distorts result. In sum, PCM predetermines the results way too in an exploratory investigation. Its use should be strongly restricted, plus omitted by default. In this light, MLFA is the recommended way to start the theory development process.

Notwithstanding, PCM should not be considered generally \enquote{bad}. A bad default default does not imply the method is itself worthless. But given what we know about the OUI, it is too rigorous to start with. However, as part of model comparison, it can be one of the most valuable practical strategies to learn from data. By contrasting models with varying basic assumptions and predicting how they will behave often generates greater model and data insights. Thus, using PCM in model comparison can prove as a valuable source of information in the learning process\endnote{Note that by varying different estimation approaches, one does usually iterate through various error assumptions \parencite{Joreskog1978}.}.

In the end, even if the assumption of independence may not be verifiable, learning about the model and understanding how it sees the data helps to grasp flaws and better understand its results. As most researchers use algorithmic procedures, understanding the robot is an essential prerequisite to gain access to the OUI. Beyond that, understanding is probably the best tool to combat method-blindness fallacy in applied research and thus to alleviate ignorance bias.

\paragraph{Additionals}

A practical concern with maximum likelihood estimation is its tendency to run into convergence issues \parencite{SantosSilva2011}. Since most implementations do not allow incorporating prior information, weak loading patterns and low communalities are especially problematic with a low number of subjects within each factor. This is the so-called \textsc{number of subject problem} \parencite{Revelle2021}. It involves the question of how many subjects are needed to get stable estimation results; but the last two decades of applied research missed providing an unequivocal answer \parencite{DeWinter2012, Jackson2001}. However, maybe there is a single unsatisfying one-size-fits-all recommendation: More is always better. To be precise, first, researchers should get as many subjects as possible, preferably tweaking the subject-to-factor ratio \parencite{Costello2005, Kline2014, MacCallum2001, Pearson2010, Zhao2009}. Second, they should get numerous \enquote{good} indicators, where, third, \enquote{good} means high communalities \parencite{Hogarty2005, Jung2011, MacCallum2001}. This assessment may seem vague but comes as close as one can get to determine appropriate and applicable requirements for a wide range of factor analyses.

Nonetheless, from an applied researcher’s perspective, the recommendations probably sound like statistical fairytales. Often researchers have no choice but to address problems with the data at hand. So how to deal with emerging (convergence) issues? An easy judgment rule for MLFA is to not trust a result that has not converged. Parameter estimates are too volatile. Under those circumstances, generalized ordinary least squares (OLS) estimation procedures (minimizing the error of reproduction) are good alternatives to the ML approach. The same is true if \enquote{normality of errors} is no reasonable assumption \parencite{MacCallum2012}. OLS approaches mark a great place for the reader to restart \parencite[see, e.\,g.][]{Mair2018, Revelle2021}. However, in the absence of convergence issues, when it is reasonable that errors follow a normal distribution, maximum likelihood estimation (or one of its robust extensions) is a good default.

\subsubsection{Rotation problem}

When extracting factors with MLFA, the robot delivers the most plausible values (given the data) for reproduction problem. Although the result often suffice robots, it is not so for researchers. Why? Even though the solution describes the association between factors and indicators (${\hat\Lambda}$); the result is often not in a shape that can be interpreted straightaway. The loading picture is often too diffuse. Or, in other words, the loading matrix is not in a simple structure\endnote{\textcite{Thurstone1935, Thurstone1947, Thurstone1969} developed a set of principles to guide the rearrangement of the loading matrix ($\hat{\Lambda}$). Thinking in zeros and ones, one aims to redistribute the zeros and ones in the loading matrix to achieve an unequivocal loading pattern. At best, every item loads only on a single factor. If this is true for every item, the outcome has literally a simple structure. Each item is clearly attributable to (i.e., loads highly on) a single factor: $\exists \xi_i, \xi_j:\lambda_{a_i}, \lambda_{b_i}, \lambda_{c_i} \gg \lambda_{a_j}, \lambda_{b_j}, \lambda_{c_j}$. For \textcite{Carroll1953, Tucker1955, Jennrich1966}, and others, this must-have been one of those statistical fairytales. They all criticize that a \enquote{Simple Structure} will be hardly ever reached. Nonetheless, nowadays there are various implementations to come close to the ideal. Some of them will be discussed afterward.].This is at the heart of the rotation problem. The key question is how to transfer the robot’s intermediate to something simple -- which means more interpretable ($\hat\Lambda_r$).}.

Transformation often overcomes the problem. A transformation of the loading matrix is typically referred to as \textsc{rotation}. Rotations are literally interpretation aids for researchers, trying to produce unambiguous loading pictures while maintaining the extracted relationships between variables in the result. Their outcome usually yields a clear overall picture of the factor-item correlations ($\hat\Lambda_r$). Ultimately, rotations foster clarity, by facilitating the broader understanding of the patterns themselves. A simpler (i.\,e., interpretable) structure can be achieved in two ways; research can either utilize orthogonal or oblique rotation techniques. Both will be discussed after shedding light on the researcher’s and robot’s perspectives.

\paragraph{Researcher’s perspective}

To get an idea of what it means to rotate, imagine a Cartesian coordinate system (\autoref{fig:facord}). For ease of sake, imagine the space is two-dimensional. With two dimensions, there are two axes $X$ and $Y$. Re-label $X$ and $Y$ with the labels of two factors, for instance, “honor” and “pride”. In addition, restrict the range of values for $X$ and $Y$ to stay within the one-minus-one interval. Why minus one to one? Because the items are now defined in terms of their loadings and loadings, in turn, are factor-item correlations. Let’s say the item \enquote{You would praise a man who acts aggressively to insult} is located at $I: (0.5,0.6)$. If the values 0.5 and 0.6 represent the item’s correlations with each factor ($\hat{\lambda}_{honor}$; $\hat{\lambda}_{pride}$), one can say \enquote{You would praise a man who acts aggressively to insult} correlates moderately to highly with \enquote{honor} and highly with \enquote{pride}. By generalizing this logic, the entire loading matrix ($\hat{\Lambda}$) can be mapped into a two-dimensional space using factor coordinates.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/facord-1.png}
\caption[Factor Coordinate System]{Factor Coordinate System -- a visualization of a loading matrix. The loadings allow mapping the 10 simulated variables in the coordinate system. The average loading is $\bar\lambda=.55$ and the between-factor correlation $\phi_12=0.2$ }
\label{fig:facord}
\end{figure}

But does the item now relate to \enquote{honor} or \enquote{pride}? This is just it; since both values are approximately equal, one cannot easily say. As mentioned above, tackling the issue implies translating the intermediate into something simple. In the graphical example, the question formulates as follows: How to reallocate the axis in the coordinate system to generate the desired simple(r) loading structure? There are mainly two choices: either to \enquote{spin axes}, using orthogonal rotation techniques that maintain the right angle between axes; or to apply an oblique transformation, which allows the factor axis to move independently. Hence, in the latter, the factors can correlate with each other.

First, for understanding \textsc{orthogonal rotation}, the key question is how to reallocate the axis that a simple (i.\,e., easy-to-interpret) loading-structure results. Think of the solution in terms of a spinning rotor. The rotor represents the factor coordinate system. The bolt (i.e., the origin) holds the rotor in place so that the blades (i.e., axes) can spin. When the rotor blades turn, they maintain a fixed 90-degree angle. The same holds true for factor rotation. When spinning the axes in a factor coordinate system, the blades move counterclockwise, preserving the right angle between them. Mainly, one refers to the procedure as \enquote{orthogonal rotation}.

The metaphor breaks, second, in the oblique case. Here, one does not spin the axes, preserving the right angle between. The crux is to explicitly allow the axis to abandon perpendicularity. Intuitively, the two-dimensional oblique case is better thought of as a clock (i.\,e., a factorial coordinate system) in which the hands (i.\,e., the factors) may move independently. Thus, the “time” becomes relevant, (i.\,e., the angle between both factors), because it provides additional information on between-factor correlations.

Next, it will be shown how graphical rotation builds up on matrix algebra. Indeed, a visual outline is helpful to start with, though impede with a full understanding of the comprehensive modeling approach. This holds especially true for modern oblique techniques. To distinguish the graphical and algebraic approach the term \enquote{oblique transformation} \parencite{Revelle2021} will be uses afterwards. Nevertheless, the question remains; what happens inside the machinery? So let’s change perspectives to get under the hood.

\paragraph{Robot’s perspective}

As it will be shown, from a robot’s point of view, rotation problems are a matter of solving the transformation equation under different constraints. Any solution, however, builds upon the \textsc{Transformation equation}, formalizing as:

\begin{equation}
\hat{\Lambda}_r = \hat{\Lambda} {T}
\end{equation}

The first thing to notice is, once more, the equation is just the formalized versions of the aforesaid obstacle. Namely, finding a set of axes that produces a simpler loading structure ($\hat{\Lambda}_r$). More precisely, the solution just \textit{appears} simpler (i.\,e., more accessible for the researcher), because the actual relationships between variables are left untouched. To put it another way, the robot searches a transformation matrix (${T}$) that rearranges the estimated information ($\hat{\Lambda}$), but does not change the model’s fit to the data \parencite{Mair2018}. In this light, the formal equivalent of spinning the axis is a multiplication with a transformation matrix (${T}$). As such, the transformation matrix is the pivot point in the above’s equation and key to understanding transformation procedures in general. But what particularly happens inside the machinery?

In the orthogonal case, the plotted matrix of factor loadings ($\hat{\Lambda}$) is multiplied with a transformation matrix ($T$), which moves the axes counterclockwise until the desired result ($\hat{\Lambda}_r$) is reached. Thereby, the robot has to find a solution with two important properties: First, obviously, the transformation ($T$) should be able to solve the transformation problem ($\hat{\Lambda}_r = \hat{\Lambda} {T}$). But even more important, second, is the premise to preserve the right angle between factors. Mathematically, a corresponding solution can be achieved with a set of orthogonal vectors, because their orthogonality allows to maintain the right angle -- rotating the factors (counterclockwise) by 90 degrees. This is done assuming $TT^t=I$\endnote{Once introduced, orthogonality maintains in a system. This holds especially true, modeling the information with the eponymous rotation. Starting (1) from the fundamental theorem, using (2) the transformation equation and (3) the orthogonality constraint, one can easily show how orthogonality preserves. Accordingly, the derivation is called the \enquote{law of conservation of orthogonality}:
\begin{gather*}  
P_r = \Lambda_r\Lambda_r^t + \Psi\\
P_r = \Lambda T (\Lambda T)^t + \Psi \\ 
P_r = \Lambda\underbrace{T T^t}_{I} \Lambda^t + \Psi \\
P_r = \Lambda\Lambda^t + \Psi  \\ 
P_r = P
\end{gather*}
}.

In the oblique case, solving the rotation problem implies resolving the perpendicularity of the axes (algebraically, $I \neq TT^t$). By giving up on the assumption of factor orthogonality, one preserves and estimates the matrix of factor correlations (${\Phi}$). The matrix of factor correlations is part of the fundamental equation of factor analysis ($P = \Lambda \Phi \Lambda^{t} + \Psi$) and contains the correlations between factors. For instance, if $\Phi_{\phi_1; \phi_2} = 0.6$, where $\phi_1$ represents \enquote{honor} and $\phi_2$ represents \enquote{pride}, the data include a high correlation between \enquote{honor} and \enquote{pride}. Using orthogonal rotation instead, factor-correlations are assumed to be 0, and $\Phi$ is omitted\endnote{To shed light on this, let\u2019s look at the pattern-structure-matrix distinction. In research literature, the terms \textsc{structure matrix} and \textsc{pattern matrix} are omnipresent. To build an intuition, the structure encompasses all common information in the correlation matrix. (1) The information between items and factors, as well as (2) the information between factors. The decomposition is used in the fundamental equation ($P = \Lambda \Phi \Lambda^{t} + \Psi$). If the factors do not share any additional information, their correlation is zero ($\forall \phi_{i,j | i \neq j} = 0$) and $\Phi$ becomes the identity matrix $I$. Here, the structure can be reproduced using only factor-item information. So in the end, the structure (${S}$) is reproducible as a function of the factor-item information (${\Lambda}$) and factor-factor information ($\Phi$). Algebraically, this can be written as: $S = \Lambda \Phi$. So rotating orthogonal sets of the correlation between factors, the off-diagonals become zero, $\Phi$ equals $I$, and $S$ reduces to $\Lambda$. Structure matrix and pattern matrix are identical. The key question in the following will be how realistic the assumption of independence really is in applied social science research.}.

\paragraph{Solutions}

As it has already been said; when solving the rotation problem, researchers have mainly two choices
\endnote{A technique which was always part of exploratory analyzes since the prior work of Thurstone in the 1940s is visual rotation. The problem with it is that selecting factor radians ($\theta$) imposes some kind of arbitrariness in the rotation procedure \parencite{Carroll1953}. Researcher are often accused to miss a concise \enquote{objective} argument on which to evaluate the position of the axes. Hence, researchers usually turn towards analytic criteria. What often remains unnoticed is, visual rotation allows a pretty good approximation of a simple structure \textcite[p. 202]{Gorsuch2015}. However, nowadays, the use of analytic criteria predominates the social sciences. Most of them are oriented towards providing a mathematical gateway to a simple structure. Thus, today’s focus is on how to minimize a matching criterion}: orthogonal rotations or oblique transformations. When it comes down to a decision, researchers have to evaluate if it is reasonable to ignore the between-factor correlations or if they need to be modeled explicitly. This is crucial to the OUI in the social sciences because the first option keeps perpendicularity, which is another appearance of factor independence. Thus, \textit{the decision between orthogonal and oblique rotations has a much bigger effect on the OUI than choosing within the sphere of orthogonal or oblique criteria}. That is why the particular solutions are clustered with respect to their origins (i.\,e., orthogonal or oblique). Today, there are over two dozen rotation procedures \parencite[pp. 214]{Gorsuch2015} -- too many to discuss all of them. However, some of them (i.\,a., the bad default, and the most decent alternative) are elaborated in the following sections.

\subparagraph{Bad defaults}

Among orthogonal rotation techniques like quartimax \parencite{Carroll1953}, equamax (Saunders, 1962)\endnote{The original paper from Saunders \enquote{Transvarimax: Some properties of the ratiomax and equamax criteria for blind orthogonal rotation} is not accessible. Drawing on \parencite{Gorsuch2015}, the proposed method was documented in a paper delivered at the American Psychological Association meeting in 1962. Additional information about equamax can be found in \parencite{Kaiser1974a}.}, and orthomax \parencite{Harman1970}; varimax \parencite[]{Kaiser1958} is the most popular choice in applied research \parencite{Costello2005, Ford1986, Loo1979}.

\textcite{Kaiser1958}'s \textsc{Varimax} criterion proposes a solution (${T}$) for the rotation problem $\hat{\Lambda}_r = \hat{\Lambda} {T}$, by maximizing the variance of the squared loadings for each factor ($\xi$). This means varimax is out to boost the (squared) correlations between factor and items ($\hat{\Lambda}\hat{\Lambda}^t$). In doing so, the transformation ($T$) makes the factor axis rotate by 90 degrees counterclockwise, preserving the right angle between the factor-vectors\endnote{Technically speaking, maximizing the sum of squared loadings ($\hat{\Lambda}\hat{\Lambda}^t$) on each factor ($\xi$) actually leads to a transformation matrix (${T}$), which multiplies the columns of the matrix of factor loadings ($\hat{\Lambda}$) by radians ($\theta$) \parencite{Revelle2021}. Thereby, they rotate (counterclockwise) by $\theta$.}. $T$ spins the axes (counterclockwise), so to speak. Accordingly, Kaiser’s varimax rotation subsumes under the orthogonal rotation techniques.

But why is it a bad default? To grasp the problem, it is useful to understand how orthogonality of factor-vectors relates to the assumption of independent factors. Mathematically, they are connected through the cosine. The cosine concentrates the 360-degree spectrum to a one-minus-one range. Therefore, the angle, which shows their relationship graphically, can be transferred in a well-known sphere of relatedness -- their correlation. As a consequence, if factor-vectors are perpendicular, they should not correlate. And indeed, since $\cos(90)$ equals 0, they do not. Thus, maintaining the 90-degree angle while rotating preserves the right angle and consequently factor independence. A cursory note on previous findings was already given in varimax’ explanation. varimax maximizes the variance of the squared loadings. Thinking back on the syringes in the extraction problem, varimax acts accordingly; it maximizes information density within each factor ($\hat{\Lambda}\hat{\Lambda}^t$) -- but at the cost of omitting between factor correlations ($\Phi=I$). Correspondingly, the result is the same: factor independence.

But again, there is usually no reason for the researcher to assume independence between factors -- especially not by default. \textcite[p.3]{Costello2005} urges furthermore to expect variation among factors, because \enquote{behavior is rarely partitioned into neatly packaged units that function independently of one another}. The quote should sound familiar, recalling the characteristic features of the OUI (e.\,g., blurry boundaries). Latent dimensions nearly always share some bits of information. Accessing them through independent factors is a recipe for doom, which furthermore leads to ignorance bias \parencite{Loo1979}. This is in line with \textcite{Sass2010}, who conducted a comparative investigation of different rotation criteria within EFA. They encourage researchers to critically reflect on their rotation choices since they have a significant impact on the manifestation of the factor structure. The habitual use of orthogonal rotations like varimax as the default behavior is not harmless. It is reasonably a bad default and can prove as a dangerous undertaking, especially if factors demand to correlate with each other, but chosen options hinder them to do so \parencite{Loo1979}. That’s the reason why switching to decent alternatives is mandatory.

\subparagraph{Decent alternatives}

Recall, the main goal of rotation and transformation procedures. They aim for transferring the initial results into something simple (i.\,e., easy to understand). To judge simplicity among criteria, simplicity indexes come into play \parencite[see, e.\,g.][]{Bentler1977, Kaiser1974, Lorenzo-Seva2003}. Although they differ in terms of their mathematical formulation, conceptually all \textsc{simplicity indexes} aim to find a solution with each item indicating the least number of factors. In this sense, simplifying the result maximizes interpretability\endnote{Technically speaking, the transformation (${T}$) should distribute the communality of each item across the least number of factors ($\xi$). This generates the easiest loading picture possible. Why? Because each factor-loading $\hat{\lambda}$ is either zero or as far from zero as possible \parencite{Lorenzo-Seva2003}.}. 

Usage of the most popular oblique transformation procedures, like oblimn and promax \parencite{Hendrickson1964} can be justified by scoring well on one of above's criteria. Ultimately, all solutions aim to maximize factor-simplicity and avoid factor independence. So all of them are indeed decent alternatives. But when it comes down to find the most decent alternative, oblmin, as well as promax, do not provide the simplest result possible. Indeed, they all score high on some indexes; but those are the flawed ones \parencite{Lorenzo-Seva2003}\endnote{Conceptually, the argument of \textcite{Lorenzo-Seva2003} is that other indexes partially miss the target (i.\,e., the simplest result possible). They deal with the loading merely indirectly. Bentler’s index, for example, builds upon the columns of the matrix of factor loadings ($\Lambda$). The loading simplicity, on the other hand, puts the focus on the values of the loadings directly.}. According to \textcite{Lorenzo-Seva2003}'s more accurate loading simplicity (LS) index, simplimax \parencite{Kiers1994} is a decent choice, since it outperforms more elaborate versions of oblimin and promax\endnote{The workhorse in \textcite{Kiers1994} algorithm is $\sigma(T,\Lambda_r) = || \Lambda T - \Lambda_r ||^2$, with pattern matrix ${\Lambda}$, transformation ${T}$, and target ${\Lambda}_r$. The crux is to find the so-called \textsc{best simple target}, which means to solve for a transformation (${T}$) that allows rotating the pattern matrix ${\Lambda}$, such that the rotated matrix (${\Lambda}_r$) has a given number ($p$) of zero entries.}. 

In the end, by scoring high on the LS index, the simplimax algorithm delivers a transformation matrix (${T}$) pooling the communality of each item on the fewest number of factors possible. This is why the rotated loading pattern ($\hat{\Lambda}_r$) is as simple as possible -- factor-loadings ($\hat{\lambda}_{i=1,\dots,m}$) are either zero or as far from zero as possible. In conclusion, simplimax is the most decent alternative. Given the LS-index, it proved to deliver the simplest (i.\,e., most interpretable) solution possible, while keeping track of the characteristic features of the OUI.

\paragraph{Research recommendations}

To emphasize it once more, \textit{concerning the OUI in the social sciences, choosing between orthogonal an oblique technique is weightier than choosing a particular criterion within each framework}. As it was argued, deciding for oblique transformations is a better default. Factor independence is an exertion rather than the rule, and oblique rotations allow modeling factor dependencies explicitly. They permit factors to correlate with one another, while still simplifying the patterns in the loading matrix as much as possible. Oblique transformations do no harm the OUI. With orthogonal rotation, the same is not unconditionally true.

Orthogonal rotations are ever a common choice in the social sciences. Thus, smuggling in additional information (i.\,e., factor independence) in an exploratory investigation turns out to be a serious problem in applied research. \textcite{Loo1979} stresses this point. As one of a few, he reviewed the (clinical) literature, questioning the appropriateness of the assumption of independence. \textcite{Loo1979} found, researchers fall back on orthogonal rotation procedures regularly. But noteworthy is, second, in most reviewed cases -- given background knowledge -- the assumption of independence was unreasonable. Over twenty years later, orthogonal rotations are still at the top of common-go to methods in applied research \parencite{Ford1986, Costello2005}. Thus, it is difficult to quantify, how many published research papers are flawed by the implicit assumption of orthogonality\endnote{\textcite{Gorsuch2015} prompts towards the rare case of \textcite{Guilford1981}. Replacing orthogonal with oblique transformation, he started reanalyzing his past research, finally resolving the assumption of independence.}.

For this background, it seems questionable why researchers such as \textcite{Bortz2016} are concerned with a loss in compression when going oblique. Again, the major goal in the social sciences is to compress meaningfully, not maximally. So even though some informational redundancy is induced, allowing the factors to correlate with each other; it may prevent the researcher to cope with highly distorted results \parencite{Sass2010}.

But despite their shortcomings, orthogonal rotation methods should not be excluded from exploratory investigations. Previous sections suggest only to \textit{start with the least, not the most rigid model assumption}. Learning from data implies adjusting a model to the data, not imposing a well-known model on the data. In the rotation or transformation context, it means to model inter-factor dependencies (${\Phi}$) by default and learn about them on the go. This is in line with \textcite{Sass2010, Muthen1984} who recommend rotation procedures providing a simple solution, but not at the cost of inducing incompatibility between methods and the object under investigation. The bottom line is to still find the simplest result possible. But researchers must avoid the current practice of finding simplicity at any cost.

Orthogonal rotations are not worthless in applied social-science research. One can use them strategically. For example, to highlight the added value of resolving the assumption of independence. Research should become experimental, trying different techniques and evaluate their scientific use by learning from model differences and similarities \parencite[p.642]{Tabachnick2007}. No matter if results are equal across trials or completely different, in an exploratory stage of the investigation, every finding is valuable information about the models and how they see the data. Those kinds of information should not be omitted, suppressed or abandoned on a preliminary ground. It should be reported to improve domain knowledge\endnote{A four-step procedure of fitting, evaluating, understanding, and communicating deviations with the results is often part of so-called \textsc{sensitivity analyzes} \parencite[]{Saltelli2002}. Sensitivity analysis plays a special role if the object under investigation is of social or political significance.}. 

\paragraph{Additionals}

There might be scenarios in which orthogonal solutions are a reasonable choice. Thus the small additive provides some guidance on how to use orthogonal rotation techniques in applied research. In general, varimax is reasonable if factor dependence is a minor issue or if the researcher expects more than a single general factor to underlay a set of items \parencite[p. 195]{Gorsuch2015}. But if the researcher anticipates a single factor, varimax becomes problematic, because it distributes the variance across factors and thus dampens the tendency of a single factor to occur in the result \parencite{Sass2010}. As a result, if one anticipates a single general factor, quartimax is the better choice \parencite{Mair2018}. Despite that equamax, combines quartimax and varimax, portioning the variance more evenly across factors \parencite[p. 214]{Gorsuch2015}. If it comes down to a single choice for a particular criterion, authors like \textcite{Gorsuch2015} recommend varimax, because in visual inspections, they produce interpretable results and prove invariant across a wide range of circumstances.

The second addition is devoted to the \textsc{gesture of proposing}. In applied research, dimensionality analysis is often thought of and taught as if there has to be a definitive result. But especially in the early stages of exploratory investigations, there might be more than one plausible solution compatible with the constraints (e.\,g., data). In case of doubts, researchers should start to propose different plausible solutions to the research community \parencite[for an exception, see][]{Timmerman2017}. This is in line with \textcite[p. 224]{Gorsuch2015} who states that any attainable result in an early stage of theory development is an intermediate, a hypothesis, for (follow-up) investigation yet to come. As will be shown, his suggestion holds especially true, when it comes down to determine \textit{the} number of factors.

\subsubsection{Number of factor problem}

Conceptually the easiest and at the same time the most striking problem in factor analysis is the number of factor problem. The obstacle is literally, to find the appropriate number of factors to extract. It is so striking because misspecification distorts results \parencite{Velicer2000}. Two scenarios proved troublesome in applied research: (1) \textsc{underextraction} -- pulling out too few factors and (2) \textsc{overextraction} -- pulling out too many of them.

\textcite{Fava1996}, for instance, show underextraction evoking biases when inferring results from distorted patterns ($\hat{{\Lambda}}$). Furthermore, pulling out too few factors leads to \textsc{omitted factor bias}. \textcite{Hayton2004} argue omitting factors induces a loss of information, which compares to a loss in theoretical significance. But typically the most common decision-making tools in applied research tend to overextract, they add. Therefore, pulling out too many factors seems far more common in applied research.

Overextraction is problematic because overshooting the number of factors introduces spurious ones, leaving researchers with over-saturated models. As a result, model outcomes are often too complicated to understand \parencite{ Fabrigar1999, Wood1996, Zwick1982, Zwick1986b}. Although overextraction is more common in applied research, both misspecifications blur insights and impedes gaining a true understanding of underlying structures. Distorted solutions furthermore hamper the ability to interpret results meaningfully and thus mislead latent-variable inference. This is why a lot of research has gone into solving the number of factors problem. The outcome has been a bunch of \textsc{retention criteria} proposing a plausible range for the number of factors to extract. With a focus on bad defaults choices and decent alternatives, some of them will be presented after shedding light on the researcher’s and robot’s perspective of the problem.

\paragraph{Researcher’s perspective}

Rephrasing above’s problem description, the number of factors can be said to affect the accuracy to reproduce the information in the correlation matrix directly. Researchers find themselves in a \textsc{simplicity-accuracy trap}: Whereas a smaller number of factors contributes to parsimony, a larger number of factors contributes to a more accurate description of the phenomenon of interest \parencite{Revelle2021}. At maximum, the number of parameters ($|\xi|$) and indicators ($|X|$) are equal and the information in the model-implied correlation matrix ($P$) is simply a \enquote{factorial re-description} of the given structure in the input correlation matrix ($R$): $|\xi| = |X| \Rightarrow P = R$. In other words, there is no compression but a bunch of factors mimicking indicators to re-describe the structure. But if researchers seek latent-variable inference, they need to compress meaningfully. So the key question is how to navigate between simplicity and accuracy?

Even though advice is hard to give advice, probably \textit{truth} lies somewhere in between a full model and a null model. Researchers may avoid \enquote{structural mimicry} by choosing the full model, allowing to transmit all structural information, but at the cost of simply re-describing what is already known. At the same time, they may question the use of overly simple models, because large data sets are usually pervaded by -- often even more than two -- latent dimensions.

Ultimately, researchers must realize that \textit{samples are flawed, incomplete representations of true patterns and data generating processes} -- latent dimensions. Why? Because then it might become obvious that not every piece of information in the correlation matrix is worth extracting. Researchers should aim for the common features -- the ones which can be found across samples. From this point of view, \textsc{full encryption} of the sample distorts results by learning too much from the sample, whereas \textsc{null-encryption} distorts results by ignoring relevant features of the data \parencite[p. 192]{McElreath2020}. Researchers must navigate between the two extremes. Thus, from their perspective, the number of factor problem is rather an optimization problem. Researchers have to find the minimum number of common factors decrypting the maximum amount of information.

\paragraph{Robot’s perspective}

From a robot’s perspective, the researcher’s problem (i.e., to find the minimum number of common factors decrypting the maximum amount of information) reads as follows:

\begin{equation}
\min p : P \rightarrow R
\end{equation}

 The first thing to grasp is that the number of factor problem is almost independent of the factor analytic robot. Ordinary FA does not solve for the minimum number of factors ($p$), so to say\endnote{In recent developments, like \textsc{Bayesian exploratory factor analysis} (BEFA; \cite{Conti2014}, the algorithm proposes a set of solutions rated according to their plausibility. This means BEFA rates the different number of factors according to their posterior probabilities. Regarding MLFA, the result also includes the most plausible values given the constraint. But constraints are furthermore data and prior knowledge.}. In the following, it will become clear; the information on how many factors to consider mostly flows from external sources into the analysis. This is where statistical and mathematical criteria set in.

However, at first, the finding clarifies the role of the robot in FA. It delivers a result (${P}$) to reproduce the common information in the correlation matrix (${R}$) only given pre-specified input ($i$). Correspondingly, it is the researcher proposing a possible number of factors ($p$) to retain. The robot only delivers. It is just a servant inspecting the correlation matrix under some given set of constraints (e.\,g., $p=i$). As a consequence, the formalization of the problem re-renders as:

\begin{equation}
R \rightarrow P \, | \, p=i
\end{equation}

Against the backdrop, FA follows a master-servant model in which the appropriate number of factors is found in a trial-and-error approach. Researchers must realize their important position in factor analysis. They are masters, instructing their robotic servants as follows: (1) The researcher requests a result ($P$) with a particular value ($i$) for the number of factors to extract ($p$), (2) the robot delivers the requested result ($P | i$). (3) The researcher evaluates the outcome ($P | i$). If (4) the researcher approves the solution ($p=i$) the search is over; else (4=1) they insert another possible number of factors to extract ($p = j | j \neq i$) and restart the process.

Under the aforementioned process, researchers can come close to an appropriate number of factors to extract. They iterate through possible solutions manually, following the logic; if there is insufficient accuracy in the result, another factor should increase the goodness of approximation \parencite[p. 152]{Gorsuch2015}. So researchers increase the number of factors successively, improving the model’s goodness of fit: $p_i > p_j \rightarrow Pi > Pj$. The ideal place to stop is when improvements become negligible. If this point is reached, they solved the optimization problem optimally; finding the most parsimonious and at the same time most sufficient result possible ($\min p$).

Even though the outlined strategy solves the parsimony-accuracy trap theoretically, executing the strategy becomes a Herculean task -- especially for increasingly large data matrices. One has to request and evaluate $p = m$ possible solutions. Thus, pragmatically seen, some guidance proves helpful. This is where mathematical and statistical criteria come in. They help to find a plausible \textit{range} of possible solutions for the number of factors to extract. Focusing on bad defaults and decent alternatives, some of them are presented right away.

\paragraph{Solutions}

There are numerous implemented solutions to solve the number-of-factor problem \parencite[see e.\,g.,][]{Revelle2021, Timmerman2017}. \textcite{Revelle2021} tells the anecdote of Henry Kaiser who invented \textit{a} criterion every morning before breakfast; knowing that the problem is one of inventing criteria for \textit{the} solution. On that account, the focus will be shifted to only a subset of criteria shown to provide sufficient results.

\subparagraph{Bad defaults}

The most common ad hoc criteria used in applied research are the scree test \parencite[]{Cattell1966} and the eigenvalues-greater-than-one (K1) criterion [@Kaiser1960]. Both methods propose a cut-point for the number of factors to retain and are still today’s defaults in statistical software like SPSS. As such, they are common go-to methods in applied research \parencite{Hayton2004, Taherdoost2014}.

\subparagraph*{K1}

The idea of K1 \parencite{Kaiser1960} is to extract principal components as long as each of them pools together more information than a single item in the data set. More technically, it stops iterative extraction of further components\endnote{\textcite{Mair2018} notes that the K1-solution is based on an eigenvalue decomposition of the correlation matrix (${R}$). Correspondingly, the Kaiser criterion provides a solution based on the principal component (PC) model. As a result, it iteratively extracts \textit{principal components} which explains successively less variation in the data -- until the eigenvalue of a standardized variable is reached. For that reason, \textcite{Timmerman2017} subsume K1 under the PCA-based criteria.} only if their variance -- the sum of the squared factor loadings ($\Lambda\Lambda^t$) -- falls below the variance of a standardized item. That’s where the “eigenvalue-greater than-one criterion” gets its name from; eigenvalues of standardized items have value “1”. Correspondingly, K1 can be said to extract principal factors as long as one of them exceeds an eigenvalue of 1, too.

Reviewing past literature\endnote{Because of its flaws, nowadays, there is almost no research effort put into the Kaiser criterion. Reviewing the literature, there are only improved versions of the classical criterion, like the empirical Kaiser criterion \parencite[]{Braeken2017}, under review \parencite[see][]{Auerswald2019}.}, the downfall of K1 can be justified based on its erratic and abysmal suggestions \parencite{Lee1979, Tucker1969}. More particular, it showed mixed tendencies to underextract and overextract \parencite{Linn1968, Yeomans1982}. Its poor performance is often attributed to small sample sizes per factor \parencite[]{Browne1968} and low communalities \parencite{ Zwick1982, Zwick1986b, Hakstian1982}. However, \textcite{Revelle1979} as well as \textcite{Velicer1990} underline K1’s tendency to overextract. \textcite{Revelle2021} furthermore proposes to divide K1’s suggestion by three, since results turn out to be more realistic with the adjustment.

Besides its tendency to overextract, K1 suggests a number of factors upon a model including the assumption of independence \parencite[]{Lorenzo-Seva2011, Timmerman2017}. As a consequence, researchers should expect deviating results if they set up a common factor model under those constraints. But in absence of systematic investigations, it remains unclear if (and how) the assumption of independence leads to actual differences in recommendations for the number of factors to retain. Theoretically, however, deviations might arise if the assumption of independence does not hold. Why? Because, a suggestion based on a model resting on independence can differ vastly from a model which resolves the assumption explicitly. These deviations, in turn, should influence the result ($p$) -- which is the suggestion for the number of factors to retain\endnote{The argument is derivable, following the fundamental equation: $P = \Lambda \Phi \Lambda^{t} + \Psi$. If a model ($P’$) assumes factor-independence ($\Phi = I$) and P’ simplifies to the independent version of the fundamental equation: $P’ = \Lambda I \Lambda^{t} \Psi$. In that sense, P’ equals P the more ${\Phi}$ equals ${I}$, which means, the more factor independence is reasonable. Formally, this can be written as: $P \approx P’ | \Phi \rightarrow 0$. The argument is, since, the number of factors ($p$) determines the form of the model (${P}$) deviations between the models ($P \not\approx P’$) should imply deviation in the number of factors to retain ($p \not\approx p’$). Formally: $\Phi \not\approx I \Rightarrow P \not\approx P’ \Rightarrow p \not\approx p’$.}.

In sum, K1 turn out to be a bad default for applied social science research. Using it irresponsible as a common go-to approach to determine the number of factor witnesses a method blindness fallacy which is reasonable to induce ignorance bias.

\subparagraph*{Scree test}

In Cattell’s scree test \parencite{Cattell1966} researchers examine the scree plot to determine the number of factors to retain. The \textsc{scree plot} is a graphical visualization in which the eigenvalues ($\lambda$) of the correlation matrix (${R}$) are sorted in descending order and plotted against an index ($I=1, \dots, m$). Thus, it is sometimes called an eigenvalue-index plot\endnote{\textcite{Timmerman2017} subsume the scree test under the PCA-based approaches, because it bases on eigenvalue decomposition of the correlation matrix (${R}$), too. This means, the scree test, as well as K1, provide their solutions based on the principal component model. More precisely, the model iteratively extracts components that explain successively less and less variation in the data. When there is no unexplained bit of variation left, all eigenvalues ($\lambda$) are sorted in decreasing order and plotted along with an index ($I = 1, \dots, m$). Note that, because of the decomposition strategy, there are as many eigenvalues ($\lambda_{i = 1, \dots, m}$) as there are items in the data set.}. 
The result is shown in \autoref{fig:scree}. To test for the number of factors to keep, the researcher watches out to find the cliff which divides rock and scree. This means to examine the plot for a drop in the eigenvalues -- the so-called \enquote{elbow} -- allowing to differentiate between systematic and random bits of variation visually. The proposed number of factors to retain is given by the index-value *before* the line flattens \parencite[p. 176]{Gorsuch2015}.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/scree-1.png}
\caption[Scree plot]{Scree plot of 10 simulated variables with average loading $\bar\Lambda=.55$ and between-factor correlation $\phi_{12}=0.2$. The horizontal line furthermore includes the suggestion of K1.)} 
\label{fig:scree}
\end{figure}

From an applied researcher’s perspective, a scree test proves troublesome if there is (1) either no cliff, allowing to tell apart the rock from the scree, or (2) if the eigenvalue-landscape looks like a mountain scenery, with multiple rocks, cliffs, and screes. This is in line with \textcite{Hayton2004}, who argues scree plots are manageable if there are powerful factors, but turn ambiguous in the absence of a clear drop-off or with multiple cut points. But if the number of observations and communality values increase, some ambiguity vanishes \parencite{Horn1979}. However, the previous fact cannot outweigh another in complex scenarios. scree plots are often inconclusive; especially if different inspectors evaluate more complicated plots \parencite{Crawford1979, Zwick1986b}. Thus, it is questionable if the scree tests allow for unity in the decision of how many factors to retain. Concerning the problem of misspecification \textcite{Zwick1986b} further notice the scree test’s bias towards overextraction. But because it is not directly influenced by the number of items, it misbehaves much less than K1 \parencite{Ledesma2007}.

Nonetheless, the scree test also bases on eigenvalues, hence shares its flaws with K1. The problem manifests particularly in terms of the OUI. scree tests suggest their number of factors to retain based on a model assuming independence between factors. Although deviations are not empirically evident, one should expect them\endnote{So far, the argument why PCA-based and CFA-based model differ regarding the number of factor to retain based upon factor independence. But as shown, the differences exceed the problem of independence. Both use (1) different models and rely (2) on different sources of information. \textcite{Zwick1986} did already prompt towards differences between the two strategies in terms of the implied number of factors to retain. They conclude, PCA will propose far more components than CFM do factors. The result should not surprise. Once more, PCA attempts to explain all the information in the correlation matrix. For this reason alone, there is more information to explain, logically demanding a larger number of factors.}.

In conclusion, scree plots disqualify as default retention criterion. Their flaws label them suboptimal as a go-to choice, mainly because of their erratic and abysmal suggestions. Furthermore, they propose solutions based on a model, which is incompatible with the OUI’s characteristic features. Their frequent use in the social sciences testifies existing method blindness and validates ignorance bias in applied research.

\subparagraph{Decent alternatives}

In applied science, probably the greatest achievement in the last three decades was the rapidly growing use of simulation-based approaches. Concerning factor analysis, parallel analysis \parencite[]{Horn1965} is a good example. \textsc{Parallel analysis} (PA) is an upgrade of Kaiser’s idea, incorporating sampling variation through a random process \parencite[]{Auerswald2019, Saccenti2017}. A basic PA comprises 4 steps \parencite{Hayton2004}: First, it produces ($K$) random duplicates of the original data set ($X_{n \times m}$). Second, PA determines the random eigenvalues ($\lambda^*_{j = 1 \dots J}$) from the ($K$) random duplicates ($X^*_{k = 1 \dots K}$). Third, it calculates the mean and, more recently, the 95th-percentiles \parencite[]{Glorfeld1995}, of all eigenvalues ($\lambda^*_{j = 1 \dots J}$). Four, it compares the eigenvalues from the original data set ($\lambda_{j = 1 \dots J}$) with the (implied boundaries) of the random eigenvalues ($\lambda^*_{j = 1 \dots J}$). Finally, factors are rejected if their eigenvalues are smaller than the simulated ones.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/PA-1.png}
\caption[Parallel Analysis]{Parallel Analysis -- trying to decide how much factors to retain, based upon 10 simulated variables with average loading $\bar\Lambda=.55$ and between-factor correlation $\phi_12=0.2$. The blue line show the simulated eigenvalues.} 
\label{fig:PA}
\end{figure}

About possible flaws, \parencite[]{Turner1998} noted the tendency to underextraction. However, the more recently overall performance of PA (with 95th-percentiles) has shown to be good \parencite[]{Dinno2009, Lorenzo-Seva2011, Peres-Neto2005, Steger2006}. This is in line with, \textcite{Velicer2000} who replicated the results of \textcite{Zwick1986b} in a comprehensive simulation study, ordinary PA does comparatively well. But recall that PA is an extension of Kaiser’s method. As such, it is a PCA-based approach, even though it is commonly used to determine the number of factors \textcite{Timmerman2017}.

More recent developments try to overcome previous issues and provide more widely applicable and more accurate parallel analyzes in applied research. Using PA-MRFA \parencite[]{Timmerman2011}, for instance, one tries to get two birds with one stone, (a) identifying the number of common factors in (b) a series of polytomous items. Instead of the common PCM approaches (i.e., PCA and PAFA), PA-MRFA draws on minimum rank factor analysis (MRFA) \parencite{Shapiro2002}. In a nutshell, the procedure tries to find the number of factors minimizing the unexplained bits of the common variance. More precisely, PA-MRFA does not fiddle with eigenvalues ($\lambda, \lambda^*$). It draws on more meaningful comparisons focusing on the explained common variance portion of random and empirical data \parencite{Timmerman2011}.

To additionally include the ordinal nature of most single stimulus data (e.\,g., 1: Fully disagree, $\dots$, 5: Fully disagree) polychoric correlations can be used with PA and FA. Think of polychoric correlations as bespoke correlation-measure for polytomous variables. They promise more accurate results \parencite{Timmerman2011}; but assume an underlying normal distribution for the observed variables in the data set\endnote{This assumption has mathematical roots. \parencite{Timmerman2017} pointed out that assuming the ordered categorical indicators to be the product of underlying normally distributed variables turns the polychoric correlation into an MLE for the Pearson correlation between latent variables.} \parencite[]{Joreskog1978, Muthen1984}. A strategy integrating all the aforesaid improvements is the hull method.

\subparagraph*{The hull method}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{Figures/hull-1.png}
\caption[Hull Method]{Hull Method -- The FA results bases upon 10 simulated variables with average loading $\bar\lambda=.55$ and between-factor correlation $\phi_12=0.2$.)} 
\label{fig:hull}
\end{figure}

The hull method \parencite[]{Lorenzo-Seva2011} is a recent development to determine the number of common major factors in EFA. As such, it is a bespoke alternative to the aforementioned PCA-based approaches. It builds upon a numerical model selection approach based on a convex hull \parencite{Ceulemans2006}, trying to pin down the number of factors by numerical inspection of a scree-like plot (i.\,e., $X$: degrees of freedom, $Y$: goodness-of-fit measure, \autoref{fig:hull}). Relying on calculation instead of visual inspection, the hull solution can overcome subjectivity in the choice of how many factors to keep. To cope with overextraction, it sets an upper bound calculated in an ordinary parallel analysis plus one additional factor \parencite[]{Lorenzo-Seva2011}. The \enquote{numerical elbow} is calculated at the point where increasing the number of factors leads only to a marginal, local increase in the goodness of fit. The approach is thus an elegant mathematical solution to navigate between simplicity and accuracy.

The hull method incorporates different goodness-of-fit measures, defining a particular hull variant. According to \parencite[]{Lorenzo-Seva2011} most successful (with average hit rates over 80-90\%), are (1) Hull-CFI, which bases on the comparative fit index \parencite[]{Bentler1977} as well (2) Hull-CAF, which builds upon the Kaiser-Meyer-Olin index \parencite[]{Kaiser1974}. In combination with maximum-likelihood (or unweighted-least-squares) extraction procedures, Hull-CFI should be the method of choice in applied research \parencite[]{Timmerman2017}. A recent Monte Carlo investigation furthermore testified to previous findings, and proved it to be a handy retention criterion in a researcher’s toolbox \parencite{Auerswald2019}.

A concluding remark, \textcite{Lorenzo-Seva2011} notes the hull method to perform best with a sufficient number of indicators per factor ($m_{\xi_i} > 5$). If this is not possible, ordinary parallel analysis with a 95th-percentile threshold \parencite{Glorfeld1995} is a decent alternative. Besides, \parencite{Auerswald2019} found high hit rates in orthogonal unidimensional scenarios, regardless of the sample size.

\subparagraph*{Sequential $\chi^2$-Test}

A well-suited test for MLFA integrated into most statistical software is the \textsc{likelihood ratio test} (LRT). The LRT evaluates the fit of the model under the null hypothesis of a model implied correlation matrix resembling the population correlation matrix \parencite{Auerswald2019}. Starting from a null model, the test is repeated, increasing the number of parameters successively. In the limits ($n \rightarrow \infty$) the test statistic furthermore follows a $\chi^2$ distribution \parencite[]{Auerswald2019}, which is why it is called \textsc{sequential $\chi^2$ model test} (SMT; \cite{Auerswald2019}). At some point, SMT will yield an insignificant result ($p < 0.05$). If so, increasing the number of parameter stops and the model which produced the last significant result finally contains the number of factors to retain.

However, one should also familiarize with SMT’s flaws. Replicating the results of earlier investigations, \textcite{Ruscio2012} show its tendency to overextract. Besides, a more recent study of \textcite{Green2015} also shows its tendency to underextract. Thus, recent findings are inconclusive. However, in the latest systematic investigation of \textcite{Auerswald2019}, SMT outperforms other approaches as the hull method -- especially in situations with correlated factors. Based on their Monte Carlo study \textcite{Auerswald2019} promote the use of SMT in applied research. Another minor peculiarity is SMT’s arbitrary 5\% threshold. But the comment seems negligible given its verifiable performance under those constraints. Despite that fact, no serious violations were found concerning the OUI’s characteristic features. In conclusion, the sequential model test proves a decent alternative in comparison to more rigid retention criteria.

\paragraph{Research Recommendations}

Let's start with the non-recommended bits. Today, one can to hence should avoid K1. \textcite{Patil2008}, for instance, is very clear about it, demanding to abandon the Kaiser criterion in theory development. \textcite{Velicer2000} follow the claim and additionally request to eliminate K1 as the default option in commercial stats software. \textcite{Preacher2003} looking forward to this decision, because they continuously try \enquote{[r]epairing Tom Swift’s electric factor analysis machine}. More particularly, \parencite{Revelle2021} criticizes statistical software like SPSS, because it still relies heavily on outdated criteria. From today’s view, the use of K1 can be mainly justified on historical reasons \parencite[see e.\,g.,][p. 92]{Ueberla1971}. Back in the 1960s, more elaborate and accurate approaches were computationally far too expensive. Today, with an average of 3 to 4 cores and modern i5-processors, K1 can be seen as a hero of a past war. The veteran should be praised with honor, but its time is over.

However, neither the scree test, PA, the hull method nor SMT proved faultless. Consequently, no one can be unreservedly recommended, especially if the researcher seeks out for determining the “correct” number of factors. But this is just it. Often the discussion is lead as if there is a one-and-only solution. Proposing multiple possible propositions is scarce in applied research. \textcite{Timmerman2017}, as one of a few, demonstrated more than one model to be compatible with the data. So, in general, there is nothing wrong with disagreement between criteria. Disagreement must be seen as an invitation to poke inside and understand model differences. Maybe there is more to it than misspecification. Consequently, model comparison is, once more, the recommended way to learn about models and data.

To summarize, although neither method was sufficient under any circumstances, some tools (e.\,g., hull method and PA-MRFA) proved more useful than others (e.\,g., K1 and scree test). Ultimately, there is not a single correct way to determine the number of factors. Nevertheless, there is a well-adapted opportunity to get the most out of any criteria, compensating for its weaknesses through another one. The gambit is called \textsc{combination rules} \parencite[]{Auerswald2019, Timmerman2017}. By combining different methods, researchers try to find an optimal composition of retention criteria, providing high hit rates across multiple scenarios. \textcite{Auerswald2019}, for example, found SMT and Hull to form a good duo in applied research. With respect to \textcite{Lorenzo-Seva2011}'s investigations, however, one should set up a trio. PA-MRFA is a bespoke complement fitting well into the common-factor-model framework.

\paragraph{Additionals}

A problem researchers will encounter with PA-MRFA is convergence issues. The recommended way to treat them is, again, skepticism. When parameter values haven’t been set, the results are highly volatile. Researchers should trust none of them. Bumping up the number of iterations and re-run often helps; but if not, \textcite{Timmerman2017} recommend switching back to Pearson correlations with a mean threshold.

Besides the aforementioned, there are a couple of alternative approaches which weren’t mentioned in the previous section. The very simple structure criterion (VSS; \cite{Revelle1979}), Revered parallel analysis (R-PA; \cite[]{Green2012}), the empirical Kaiser criterion \parencite{Braeken2017}, and comparison data (CD; \cite{Ruscio2012} are just a few examples. All seem promising. Therefore, they will tag a great place for the interested reader to restart. The only outlined as additional hereafter is VSS.

\subparagraph*{Very simple structure criterion}

In a nutshell, the \textit{Very Simple Structure Crtierion} evaluates the loss in quality of a factor solution when it is \enquote{degraded} to the simple structure researchers assume to be there \parencite[]{Revelle1979}. To put it another way, think of VSS as a retention criterion basing upon the \enquote{Pippi-Longstocking principle}; researchers create the loading matrix, just the way it suits them. The procedure (deliberately) fades out minor loadings -- the ones which fall below a certain limit -- to construe loading matrices as simplified versions of themselves. The simplicity of any given solution is regulated by the so-called \enquote{complexity} ($C$). The \textsc{complexity} is the number of non-zero loadings per factor: 
\begin{equation}
C=j \Leftrightarrow \xi : \exists \lambda_{i=1,\dots,j}   
\end{equation}
By manipulating it, researchers can literally design their desired solution, plugging in the number of non-zero loadings per factor (i.\,e., how simple the result should be). VSS then searches for a simplified model with the given constraints: First, it contains a pre-specified number of zero loadings which are, second, able to reproduce the original information in the correlation matrix. Consequently, the approach is practically useful. Why? Because by monitoring the simplified model’s fit, researchers can adapt to a solution that tweaks simplicity and accuracy at the same time.

\section{Factor scores (Measurement)}

The last step in a dimensionality analysis is to determine participant’s values on the latent dimension(s). In factor analysis, those are called \textsc{factor scores}. In advance of the upcoming sections, note that the focus of the thesis is on the exploratory part of factor analysis. So the topic will not be tackled with full strength.  \textcite{Grice2001a} provides a neat introduction on how to evaluate and estimate factor scores. Additionally, \textcite{Grice2001, DiStefano2009} provide a comprehensive comparison of methods to calculate them. 

There are at least two common approaches to assign factor scores ($\hat{F}$): (1) summated factor scores and (2) different regression and correlation-preserving approaches. First, recall Likert’s method from the previous chapter. Ultimately, a sum score was used to assign values after items proved reliable, according to CTT. By repeating the procedure as many times as there are latent variables, the strategy can be adjusted accordingly. The result is usually a bunch of sum or mean scores, with equally weighted (sometimes standardized) items called \textsc{unity weights}. However, the method neglects the estimated loadings completely. Omitting the loadings can be troublesome because items with relatively low loadings can partly gain significant weight. According to given circumstances, this might be a reasonable assumption, or it may not be.

In summary, \textcite{Tabachnick2007} aptly call sum or mean scores the \enquote{quick and dirty} estimate of factor scores:
\begin{equation}
\hat F = \sum_{i=1}^n X_i
\end{equation}
They are comparatively easy to calculate, although not the most we can get out of the analysis. (p. 650) Their advantage over regression estimates, for example, is to express variation in the result \parencite[]{DiStefano2009}. But if one aims for a more accurate estimate of participants’ true scores on a latent dimension, regression approaches and correlation-preserving methods have proved superior \parencite[]{Andersson2021}. Hence their name -- \textsc{refined methods} \parencite{DiStefano2009}. Conceptually speaking, factor scores are produced by pushing the information ($X$) back out through the model using $\hat{\Lambda}$ to re-weight participant’s scores ($\hat{F}$) on the latent dimension ($\zeta$)\endnote{More precisely, the regression approach is a two-step procedure \parencite[]{Mair2018}: (1) combine the correlation matrix ($R$) and loading matrix ($\hat{\Lambda}$) to estimate weights ($\hat{B}$): $\hat{B} = R^{-1} \hat{\Lambda}$ (2) Use the data ($X$), standardize them ($Z$) and push them out through the model using the score coefficient matrix ($\hat{B}$) to estimate the factor scores ($\hat{F}$): $\hat{F} = Z\hat{B}$}:

\begin{equation}
\hat F = f(\hat P) 
\end{equation}

One of the most elaborate approaches is the one of \textcite{TenBerge1999}. Leaving mathematical details aside for now, the approach is a well-suited method for applied social science research. Why? Because it provides a solution, taking into account the characteristic dependencies of the OUI. This is due to the fact that \textcite{TenBerge1999}'s estimates are a generalized approach for oblique factors, preserving their between-factor correlations \parencite[]{Revelle2021}. In addition, the procedure readjusts each item according to its loadings, incorporating a weighting scheme that regulates item influence conditional on the data.

However, care must be taken, including factor scores in further analyzes \parencite[]{Mair2018}. Recall; parameters cannot be estimated uniquely. Correspondingly, since the model cannot be uniquely defined in terms of its parameters, so are its predicted scores. The model’s indeterminacy inherits to factor scores, so to speak. More precisely, the model’s indeterminacy leads to a \textsc{factor-score indeterminacy} \parencite[]{Grice2001}. As a consequence, all approaches calculating factor scores actually produce estimates ($\hat{F}$) for participants’ values on the latent dimension \parencite[]{Revelle2021}.

A shortcoming of regression approaches is their inherent complexity \parencite[p. 283]{Gorsuch2015}.
\textcite{Gorsuch2015}, for example, bemoans that all items are included in the solution to estimate factor scores. So there might be the risk of fall for irregular features of the sample. In factor analysis, the problem is referred to as \textsc{capitalizing on chance}. \textcite{Gorsuch2015} suggests trimming the model’s possibility to capitalize on chance, by cutting the number of included items assigning equal weights. But at the same time, the suggestion misses an explanation on how to select the most appropriate ones. For the background of previously mentioned findings (i.\,a., where regression approaches outperformed sum-scores; \cite{Andersson2021}), the suggestions seem untenable. However, the problem of capitalizing on chance remains and seems not easily solvable. However, in a larger research context, the problem will be identified and diminished. In the end, the possibility to capitalize on chance is the trap-door of exploration and must be seen as a burden of learning from incomplete sources of variation -- data. This is what Sir Karl Popper once noted, saying, \enquote{I may be wrong, and you may be right, and by an effort, we may get nearer to the truth}.

\section{Example code}
 The data set used is, again, the subset of the Big Five personality traits and come from the \href{https://openpsychometrics.org/_rawdata/}{openpsychonometrics} website. 19719 participants answered 50 questions, like \enquote{I am the life of the party} on a five-point Likert scale ([1]: Very Accurate, $\dots$, [5] Very Inaccurate). To address the number of factor problem, the hull method via \texttt{hullEFA()} and parallel analysis using minimum rank factor analysis via \texttt{parallel.MRFA()} was used from the \texttt{EFA.MRFA} package \parencite[]{R-EFA.MRFA}. In addition, the sequential $\chi^2$ model test \texttt{SMT()} from the \texttt{EFA.dimension} package was applied \parencite[]{R-EFA.dimensions}. To fit the models with different rotation or transformation methods, \texttt{fa} from the \texttt{psych} package \parencite[]{R-psych} was deployed with maximum likelihood factor extraction. In addition, there is an example for the scree test and principal axes factor analysis.

\subsection*{Data preparation}

\begin{verbnobox}[\tiny\arabic{VerbboxLineNo}\small\hspace{3ex}]
# URL
url <- “https://quantdev.ssri.psu.edu/sites/qdev/files/dataBIG5.csv”
d <- read.csv(url, header = TRUE)
# Exclude: sociodemographics
X <- d[, -(1:7)]
# 0 is NA
X[X==0] <- NA
# Complete case analysis (CC)
X_cc <- X[complete.cases(X),]
# Correlation matrix
R <- cor(X)
\end{verbnobox}

\subsection*{Multidimensionality analysis}

\begin{verbnobox}[\tiny\arabic{VerbboxLineNo}\small\hspace{3ex}]
# Polychoric correlation matrix
R_poly <- psych::polychoric(X)$rho

# Number of factor problem
# Eigenvalue criterions
psych::scree(R_poly)
# Hull method
EFA.MRFA::hullEFA(X_cc, maxQ=5, extr = “ML”,
                index_hull = “CAF”,  display = TRUE,
                graph = TRUE, details = TRUE)
# Parallel analysis using minimum rank factor analysis
EFA.MRFA::parallelMRFA(X_cc, graph=TRUE)
# Sequential chi-sq test
EFA.dimensions::SMT(X, corkind=”polychoric")
# Very simple structure
n.obs <- nrow(R_poly)
psych::vss(R_poly, fm = “ml”, n.obs = n.obs)
# Rotation problem & communality problem
# Maximum likelihood factor analysis with varimax rotation
MLFA.vmax <- psych::fa(R_poly, nfactors = 2,
                       rotate = “varimax”, fm = “ml”)
# Maximum likelihood factor analysis with simplimax rotation
MLFA.simpl <- psych::fa(R_poly, nfactors = 2,
                       rotate = “simplimax”, fm = “ml”)
# Factor correlations
round(MLFA.simpl$Phi,digits=3)
# Loadings plot
# factor loadings
lambda.vmax <- MLFA.vmax$loadings
lambda.simpl <- MLFA.simpl$loadings
# Add labels
labels <- names(X)
plot(lambda.vmax, type=”n")
text(lambda.vmax,labels=labels,cex=.5)
text(lambda.simpl, labels = labels, cex=.5, col="red")
abline(h=0, v=0, lty=2)
# Principal axis factor analysis
PAFA.varimax <- psych::fa(R_poly, nfactors = 2,
                       rotate = “varimax”, fm = “pa”)
\end{verbnobox}

\subsection*{Factor scores (Measurement)}

\begin{verbnobox}[\tiny\arabic{VerbboxLineNo}\small\hspace{3ex}]
F_tB <- psych::fa(X, nfactors = 2, rotate = "simplimax",
                  cor = "poly", fm = "ml",
                  scores = “tenBerge”, missing = TRUE,
                  impute = “median”)$scores
\end{verbnobox}
