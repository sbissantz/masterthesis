\chapter{Concluding remarks} \label{chap:summary} % For referencing the chapter elsewhere, use \ref{Chapter1} 

The thesis started with a compatibility issue, namely that applied methods and the OUI in the social sciences are often incompatible with one another. Due to the (implicit) assumption of independence, researchers frequently smuggle an incompatible assumption into their analysis, which undermines the blurry boundary feature of their OUI. But as argued in the course of this paper, complex social phenomena often share common attributes, suggesting to expect correlations among them. Reckless application of common go-to methods, ignoring object-specific features, leads to distorted results -- a method-blindness fallacy imposing ignorance bias. The OUI’s characteristic features themselves suggest bad defaults and decent alternatives for the exploration process and, thus for dimensionality analysis in general. Therefore, the aim to evaluate the state of the art and find alternative approaches obliged to reconcile proposed methods for compliance with the OUI. The proposed solution was a bottom-up approach with two requirements: (1) developing the characteristic features of the OUI, and (2) elaborating profound knowledge of the methods themselves, which allows to assess and criticize them. The general strategy in this thesis consequently builds upon the \textsc{U-E-R-A} approach: \textit{understand the problem, evaluate bad defaults, replace them with decent alternatives to alleviate bias}.

Integrating \textsc{U-E-R-A} approach, this work moved from an understanding of the OUI to bad defaults and proposed decent alternatives, as well as research recommendations to alleviate bias. Moreover, the sequence of chapters and sections is structured to mimic the ongoing debate of appropriate dimensionality assessment in the social sciences. It thus worked up the topic programmatically and systematically.

First and programmatically, unidimensionality analysis was outlined and its problem to meet the large data set challenge. Second, multidimensionality analysis was suggested and the issue of assuming simultaneous contributions across dimensions from a multiple unidimensional standpoint. Finally, multidimensionality analysis was proposed as a decent alternative. In addition to the between-framework view, the debate moves on systematically within each approach. On this occasion, the aforementioned compatibility issue was included. This made it possible to select appropriate criteria and methods to combat the problem of identifying latent dimensions -- while keeping track of the OUI’s characteristic features. In total, five bad defaults were detected and replaced with decent alternatives: (1) In unidimensionality assessment, when running reliability analysis, Cronbach’s alpha has shown to be a bad default and should be replaced with coefficient omega. (2) Carrying out multidimensional assessments with factor analysis is best performed using (2a) MLFA to solve the communality issue, and (2b) simplimax to solve the rotation problem. Based on the arguments presented, PAFA and varimax rotation should be avoided, because they impute the incompatible assumption of independence into the analysis. (2c) Tackling the number of factor problem is more complex. In any case, one should avoid K1 and scree-test to determine the number of factors, because they are prone to either overextraction or underextraction -- or both. The decent alternative proposed is a combination of hull method and sequential $\chi^2$-Test. Additionally, one could top up with the very simple structure criterion. (3) To finally pin down the participants’ values on the latent dimension, one may replace sum-scores and use factor scores instead. Especially the solution of @TenBerge1999 seems promising because it incorporates between-factor correlations when estimating participants’ values on the latent dimension. It thus complements the proposed extraction and rotation strategy.

\section{Discussion}

The summary highlights the most important guidelines to conduct an exploratory dimensionality analysis. However, there is one crucial point left on the table worth discussing. In the debate on multidimensionality analysis, using FA emerged as bad default from the comparison with multiple unidimensionality and exploratory Likert scaling. Designating FA as bad default would have dramatic consequences since it is the common go-to method for dimensionality analysis in the social sciences. Are numerous papers yet flawed by the assumption of equal contributions? There are at least three significant arguments against this conclusion.

First, multiple unidimensionality makes an equally strong assumption. Recall its critique; multidimensional solutions assume each (sub-)dimension to operate \textit{simultaneously} in contributing to the observed differences between the scaled objects. Their counterargument was that in large data sets there might be multiple \textit{separate} dimensions at work, where each dimension contributes to a particular subset of the data. However, the counterargument includes an assumption, too. It is true that the multiple-unidimensional solution avoids the assumption of equal contributions though, but at the cost of assuming multiple separate contributions. Which one is more appropriate? It depends. But a priori, it is reasonable that neither one should get preference over the other. In the early stages of exploratory investigations, when prior knowledge is absent, both are equally plausible. The best thing researchers can do is model comparison. Its results are valuable information about the investigation -- either by proposing different plausible results, which are compatible with the data, or evaluating model differences trying to explain variation in the results. In contrast, selecting a particular model suppresses those insights from the research community, hence hampers the benefit to rule out potential flaws of an investigation. But researchers should take all help they can get when learning from incomplete sources of information -- data.

Second, researchers do not need to decide between options. In large data sets, it might be helpful to combine the use of multidimensional and multiple unidimensional approaches to get the most out of the analysis. There is no reason to assume both of them to be mutually exclusive. In large data sets, there might be dimensions that contribute equally to produce the observed differences between the outcomes as well as separate ones. So omitting either one neglects its benefit to explain certain parts of the whole structure. Again, there are often multiple plausible solutions compatible with the data. Learning from them requires disclosing all investigation efforts, not only parts. Selecting a model in an exploratory investigation is a decision made way too early in the “stream of information” that could justify it \parencite[]{Gelman1995, McElreath2020}.

Third, one could tackle the distinction between (multiple) unidimensionality and multidimensionality, as \parencite[pp. 81]{Jacoby1991} suggests:

\begin{displayquote}
\enquote{
[$\dots$] [T]he distinction between \enquote{unidimensional} and \enquote{multidimensional}
scaling models is essentially an artificial one. [...] In every scaling
analysis, the objective is to model the variability in the observations as
accurately as possible. This may require several dimensions, or a single
dimension may suffice. The choice between them is an empirical question. 
}
\end{displayquote}

So probably one should move beyond the goal of conducting dimensionality analysis onto a modeling approach that solely commits itself to detect interesting sources of variation. As such, factor analysis is a comprehensive modeling tool. By drawing on decent alternatives instead of bad defaults, FA can be harmonized with the characteristic features of the OUI and stays in line with the principle of compression in meaningful ways. Various implementations allow to fine-tune applications and adapt to a wide range of circumstances in applied research. Abandoning the usage of FA for artificial reasons is nonsense. The same applies to ELS. The problem with ELS from a practical point of view is merely that its current variants and implementations are underdeveloped. Nonetheless, a modern update of the algorithm will make it possible to detect interesting sources of variation in the data.

\section{Conclusion}

In the end, giving up on the artificial distinction between unidimensionality on one side and multidimensionality on the other side allows keeping two bespoke algorithms in applied research (i.\,e., FA and ELS). Both are in line with the characteristic features of the OUI, capable to meet the large-data-set challenge, and able to detect relevant sources of variation in the data. As such, they are well-tailored tools in applied research. Both should extend a modern researcher’s toolbox. Comparing their results or combining their use allows for further insights into the data. If it comes down to a final research recommendation, ELS as wells as FA can be justified. Modern extensions of FA are still state of the art, but a modern update of ELS will be an additional innovation to combat the problem of identifying latent dimensions.
